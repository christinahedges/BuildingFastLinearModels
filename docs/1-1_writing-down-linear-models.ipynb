{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7032448e-3bd5-4dc5-a3b6-fed7d3fd68dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Writing down sets of linear equations\n",
    "\n",
    "In this section, we're going to write down some linear equations using vector and matrix notation. Once we're comfortable with this we'll move onto solving equations.\n",
    "\n",
    "Linear models are quite useful because they're simple to build and will be simple to solve. Let's look at what one is, in this context. Here's an example of a linear model:\n",
    "\n",
    "$\\mathbf y = m \\mathbf x + c$\n",
    "\n",
    "Here the model $\\mathbf y$ (sometimes called the dependent variable) is given by a vector $\\mathbf x$ (sometimes called a regressor) some multiplicative factor (sometimes called a weight $m$ and an offset $c$. \n",
    "\n",
    "Let's make a model where any \"y\" value $y_i$ is given by 2 times the \"x\" value $x+i$ plus 3.\n",
    "\n",
    "$\\mathbf y = 2 \\mathbf x + 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c445764-a032-4341-94d6-684ee064b1cd",
   "metadata": {},
   "source": [
    "<div class=\"admonition\">\n",
    "<p class=\"admonition-title\">A note about noise</p>\n",
    "    <p>The true model above is better given as </p>\n",
    "    <p>$\\mathbf y = m \\mathbf x + c + \\mathbf{\\epsilon}$</p>\n",
    "    <p>where $\\mathbf{\\epsilon}$ is a noise term.  For now, we're going to drop this term, and in all this work we're going to assume that the noise is a Normal (Gaussian) noise distribution. We'll come back to this, don't worry.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df4c2e-6183-414f-81d2-a5476dd0ff38",
   "metadata": {},
   "source": [
    "### What do you mean a \"linear\" model?\n",
    "\n",
    "Above I told you $\\mathbf y = m \\mathbf x + c$ was a linear model, let's talk about what I mean by that, in this context.\n",
    "\n",
    "To fit my data, I want to build some **system of linear equations**, which are made up of regressors and shared **variables**. Here's a set of linear equations that have the same variables stored in the two elemenet vector $\\mathbf w$\n",
    "\n",
    "\n",
    "$y_0 = w_0 \\cdot x_0 + w_1$\n",
    "\n",
    "$y_1 = w_0 \\cdot x_1 + w_1$\n",
    "\n",
    "$y_2 = w_0 \\cdot x_2 + w_1$\n",
    "\n",
    "This is linear because all these equations share the same variable $\\mathbf w$. The below equation is not linear\n",
    "\n",
    "$\\mathbf y = w_0 \\cdot w_1 \\cdot \\mathbf{x} + w_2$\n",
    "\n",
    "because here's not a single solution for $w_0$ and $w_1$. \n",
    "\n",
    "## Matrix Notation\n",
    "\n",
    "Matrix notation is going to become helpful for us in this workbook. Let's express the above set of linear equations as a matrix multiplication. \n",
    "\n",
    "$ \\mathbf y = \\begin{bmatrix} x_0 & 1 \\\\ x_1 & 1 \\\\ x_2 & 1 \\end{bmatrix} \\cdot \\mathbf w$\n",
    "\n",
    "where\n",
    "\n",
    "$\\mathbf w = \\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "We could also write the first equation ($\\mathbf y = 2 \\mathbf x + 3$) as a matrix multiplication\n",
    "\n",
    "$\\mathbf y = \\mathbf A \\cdot \\mathbf w$\n",
    "\n",
    "Where \n",
    "\n",
    "$ \\mathbf w = \\begin{bmatrix} 2 & 3 \\end{bmatrix}$\n",
    "\n",
    "and\n",
    "\n",
    "$ \\mathbf A = \\begin{bmatrix} x_0 & 1 \\\\ x_1 & 1 \\\\ x_2 & 1 \\\\ .. & .. \\\\ x_n & 1 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "$ \\mathbf y = \\mathbf A \\cdot \\mathbf w = \\begin{bmatrix} x_0 & 1 \\\\ x_1 & 1 \\\\ x_2 & 1 \\\\ .. & ..\\\\ x_n & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "Putting in some values for x we find:\n",
    "\n",
    "$ \\begin{bmatrix} 3 & 5 & 7 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 1 \\\\ 2 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}$\n",
    "\n",
    "Let's implement this in `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb5de92-b1cc-4b57-aafb-00e49cf44cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[0 1]\n",
      " [1 1]\n",
      " [2 1]]\n",
      "w:\n",
      " [2 3]\n",
      "y:\n",
      " [3 5 7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Vector of x\n",
    "x = np.arange(3)\n",
    "# Matrix consisting of x, and a column of ones\n",
    "A = np.vstack([x, x**0]).T\n",
    "w = np.asarray([2, 3])\n",
    "y = A.dot(w)\n",
    "\n",
    "\n",
    "print('A:\\n', A)\n",
    "print('w:\\n',  w)\n",
    "print('y:\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46ede4-6bc6-42f1-a660-c666a125d54d",
   "metadata": {},
   "source": [
    "## Bigger Linear Models\n",
    "So long as we can write our model as something that is a set of column vector regressors with a shared set of weights, we are writing a linear model that we can solve. Your model will look like this:\n",
    "\n",
    "$y = A \\cdot w$\n",
    "\n",
    "$\\mathbf{A} = \\begin{bmatrix} x_{0,0} & x_{0,1} & x_{0,2} & .. & x_{0,n} \\\\\n",
    "x_{1,0} & x_{1,1} & x_{1,2} & .. & x_{1,n} \\\\\n",
    "x_{2,0} & x_{2,1} & x_{2,2} & .. & x_{2,n} \\\\\n",
    ".. & .. & .. & .. & .. \\\\\n",
    "x_{m,0} & x_{m,1} & x_{m,2} & .. & x_{m,n} \\\\\\end{bmatrix} $\n",
    "\n",
    "$\\mathbf{w} = \\begin{bmatrix} w_{0}\\\\ w_{1}\\\\ w_{2} \\\\..\\\\ w_{n}\\end{bmatrix} $\n",
    "\n",
    "$ \\begin{bmatrix} y_{0}\\\\ y_{1}\\\\ y_{2} \\\\..\\\\ y_{n}\\end{bmatrix} = \\begin{bmatrix} x_{0,0} & x_{0,1} & x_{0,2} & .. & x_{0,n} \\\\\n",
    "x_{1,0} & x_{1,1} & x_{1,2} & .. & x_{1,n} \\\\\n",
    "x_{2,0} & x_{2,1} & x_{2,2} & .. & x_{2,n} \\\\\n",
    ".. & .. & .. & .. & .. \\\\\n",
    "x_{m,0} & x_{m,1} & x_{m,2} & .. & x_{m,n} \\\\\\end{bmatrix} \\cdot \\begin{bmatrix} w_{0}\\\\ w_{1}\\\\ w_{2} \\\\..\\\\ w_{n}\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ecd4b-c27a-4d19-b413-79fb33a6caa7",
   "metadata": {},
   "source": [
    "A higher order polynomial is a set of linear equations. For example, let's imagine we have a vector of times $\\mathbf{t}$ for some time series. You can write a $n$th order polynomial in this \"time\" variable as\n",
    "\n",
    "$ \\mathbf{y} = \\begin{bmatrix} t_{0}^0 & t_{0}^1 & t_{0}^2 & .. & t_{0}^n \\\\\n",
    "t_{1}^0 & t_{1}^1 & t_{1}^2 & .. & t_{1}^n \\\\\n",
    "t_{2}^0 & t_{2}^1 & t_{2}^2 & .. & t_{2}^n \\\\\n",
    ".. & .. & .. & .. & .. \\\\\n",
    "t_{m}^0 & t_{m}^1 & t_{m}^2 & .. & t_{m}^n \\\\\\end{bmatrix} \\cdot \\begin{bmatrix} w_{0}\\\\ w_{1}\\\\ w_{2} \\\\..\\\\ w_{n}\\end{bmatrix}$\n",
    "\n",
    "For a 3rd order polynomial and a vector $\\mathbf t$ with four entries [6, 7, 8, 9] this looks like\n",
    "\n",
    "$\\mathbf{t} = \\begin{bmatrix} 6 \\\\ 7 \\\\ 8 \\\\ 9 \\\\10\\end{bmatrix} $\n",
    "\n",
    "$ \\mathbf{y} = \\begin{bmatrix} 1 & 6 & 6^2 & 6^3 \\\\\n",
    "1 & 7 & 7^2 & 7^3 \\\\\n",
    "1 & 8 & 8^2 & 8^3 \\\\\n",
    "1 & 9 & 9^2 & 9^3 \\\\\n",
    "1 & 10 & 10^2 & 10^3 \\\\\\end{bmatrix} \\cdot \\begin{bmatrix} w_{0}\\\\ w_{1}\\\\ w_{2}\\\\w_{3}\\end{bmatrix}$\n",
    "\n",
    "This is still a linear set of equations, even though we have a higher order polynomial, because the equations (each row) share the same $\\mathbf{w}$ vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba59f78-a966-4850-831c-d36dbc6bc7d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear equations with higher dimensions\n",
    "\n",
    "At the moment, this is one dimensional. $\\mathbf{y}$ depends on only one other variable, $\\mathbf{t}$. We can make this a higher dimensional model by including more regressors. Perhaps \"y\" depends both on time \"t\" and some angle \"$\\theta$\". \n",
    "\n",
    "If we were writing a 2D, first order polynomial in these variables our model might look like this:\n",
    "\n",
    "$\\mathbf{y} = w_0 + w_1 \\mathbf{t} + w_2 \\boldsymbol{\\theta} + w_3 \\mathbf{t} \\cdot \\boldsymbol{\\theta}$\n",
    "\n",
    "Where we have each of the variables and all their cross terms. We can write this out more explictly:\n",
    "\n",
    "$\\mathbf{y} = w_0 \\mathbf{t}^0 \\cdot \\boldsymbol{\\theta}^0 + w_1 \\mathbf{t}^1 \\cdot \\boldsymbol{\\theta}^0 + w_2 \\mathbf{t}^0 \\cdot \\boldsymbol{\\theta}^1 + w_3 \\mathbf{t}^1 \\cdot \\boldsymbol{\\theta}^1$\n",
    "\n",
    "We can represent this using matrices. Note that we must have two vectors for time and angle, and they must have the same length! Let's make up some variables.\n",
    "\n",
    "$\\mathbf{t} = \\begin{bmatrix} 6 \\\\ 7 \\\\ 8 \\\\ 9 \\end{bmatrix} $\n",
    "\n",
    "$\\boldsymbol{\\theta} = \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix} $\n",
    "\n",
    "Let's build a 2nd order polynomial for \\mathbf{t} . We'll call the matrix that holds the regressors $\\mathbf{T}$. Below I've written this as a matrix, and a set of column vectors.\n",
    "\n",
    "$T = \\begin{bmatrix} 1 & t_{0} & t_{0}^2 \\\\\n",
    "1 & t_{1} & t_{1}^2 \\\\\n",
    "1 & t_{2} & t_{2}^2 \\\\\n",
    "1 & t_{3} & t_{3}^2 \\end{bmatrix} = \\begin{bmatrix}\\mathbf{t}^0 & \\mathbf{t}^1 & \\mathbf{t}^2\\end{bmatrix}$\n",
    "\n",
    "We could dot $T$ with some weights to build a polynomial in $\\mathbf{t}$\n",
    "\n",
    "Let's build a 1st order polynomial for $\\mathbf{\\theta}$. We'll call the matrix that holds the regressors $\\mathbf{\\Theta}$ (Note that we don't have to have the polynomials be the same order..!) I've written this as a matrix and a set of column vectors.\n",
    "\n",
    "$\\mathbf{\\Theta} = \\begin{bmatrix} 1 & \\theta_{0} \\\\\n",
    "1 & \\theta_{1} \\\\\n",
    "1 & \\theta_{2} \\\\\n",
    "1 & \\theta_{3} \\end{bmatrix} = \\begin{bmatrix}\\boldsymbol{\\theta}^0 & \\boldsymbol{\\theta}^1\\end{bmatrix}$\n",
    "\n",
    "To make this a multidimensional polynomial we have to multiply the matrix $\\mathbf{T}$ by every column vector in $\\boldsymbol{\\theta}$, so we get the form we have above. This looks like\n",
    "\n",
    "$A = \\begin{bmatrix}\\mathbf{t}^0 \\\\ \\mathbf{t}^1 \\\\ \\mathbf{t}^2\\end{bmatrix} \\cdot \\begin{bmatrix}\\boldsymbol{\\theta}^0 & \\boldsymbol{\\theta}^1\\end{bmatrix}$\n",
    "\n",
    "writing this as a matrix we get\n",
    "\n",
    "$A = \\begin{bmatrix}\\boldsymbol{\\theta}^0\\mathbf{t}^0 & \\boldsymbol{\\theta}^0\\mathbf{t}^1 & \\boldsymbol{\\theta}^0\\mathbf{t}^2 & \\boldsymbol{\\theta}^1\\mathbf{t}^0 & \\boldsymbol{\\theta}^1\\mathbf{t}^1 & \\boldsymbol{\\theta}^1\\mathbf{t}^2\\end{bmatrix}$\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "$A = \\begin{bmatrix}\\mathbf{1} & \\mathbf{t} & \\mathbf{t}^2 & \\boldsymbol{\\theta} & \\boldsymbol{\\theta}\\mathbf{t} & \\boldsymbol{\\theta}\\mathbf{t}^2\\end{bmatrix}$\n",
    "\n",
    "writing this out fully (instead of as a matrix of column vectors) we see\n",
    "\n",
    "$A = \\begin{bmatrix} 1 & t_{0} & t_{0}^2 & \\theta_{0} & \\theta_{0}t_{0} & \\theta_{0}t_{0}^2   \\\\\n",
    "1 & t_{1} & t_{1}^2 & \\theta_{1} & \\theta_{1}t_{1} & \\theta_{1}t_{1}^2   \\\\\n",
    "1 & t_{2} & t_{2}^2 & \\theta_{2} & \\theta_{2}t_{2} & \\theta_{2}t_{2}^2   \\\\\n",
    "1 & t_{3} & t_{3}^2 & \\theta_{3} & \\theta_{3}t_{3} & \\theta_{3}t_{3}^2   \\end{bmatrix}$\n",
    "\n",
    "To fit this model we need a set of weights. These weights will be a vector with length number of columns in $\\mathbf{A}$.\n",
    "\n",
    "$\\mathbf{w} = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\\\ w_5 \\end{bmatrix}$\n",
    "\n",
    "In our model\n",
    "\n",
    "$\\mathbf{y} = \\begin{bmatrix} 1 & t_{0} & t_{0}^2 & \\theta_{0} & \\theta_{0}t_{0} & \\theta_{0}t_{0}^2   \\\\\n",
    "1 & t_{1} & t_{1}^2 & \\theta_{1} & \\theta_{1}t_{1} & \\theta_{1}t_{1}^2   \\\\\n",
    "1 & t_{2} & t_{2}^2 & \\theta_{2} & \\theta_{2}t_{2} & \\theta_{2}t_{2}^2   \\\\\n",
    "1 & t_{3} & t_{3}^2 & \\theta_{3} & \\theta_{3}t_{3} & \\theta_{3}t_{3}^2   \\end{bmatrix} \\cdot \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\\\ w_5 \\end{bmatrix}$\n",
    "\n",
    "Which written another way is\n",
    "\n",
    "$\\mathbf{y} = w_0 + w_1 \\mathbf{t} + w_2 \\mathbf{t}^2 + w_3 \\boldsymbol{\\theta} + w_4 \\mathbf{t} \\boldsymbol{\\theta}  + w_5 \\mathbf{t}^2 \\boldsymbol{\\theta}$\n",
    "\n",
    "i.e. our two dimensional polynomial, with shared weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3069403-feb9-423e-af0f-7ec54b735633",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Two important matrix properties\n",
    "\n",
    "Sometimes when you build linear models as you work through these notebooks, you may find that your model doesn't \"work\". Either it doesn't produce reasonable results, or Python throws an error. Here is a quick primer on two matrix properties that you will come across, which might help you debug when your model breaks:\n",
    "\n",
    "\n",
    "### What's the matrix Determinant? (i.e. why is my matrix \"singular\"?)\n",
    "\n",
    "A matrix can have an inverse. If I have a matrix $\\mathbf M$ with an inverse $\\mathbf M^{-1}$, if I dot them with each other I get back the identity matrix. Note that $\\mathbf M$ must be a square matrix.\n",
    "\n",
    "$\\mathbf M \\cdot \\mathbf M^{-1} = \\mathbf{I}$\n",
    "\n",
    "Not all matrices have inverses. \n",
    "\n",
    "If your matrix does not have an inverse it is a **singular matrix**. Here's an example of using `numpy` to calculate the inverse of a matrix using `np.linalg.inv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e2770f-94b5-495d-8b4f-c9bbf57896d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf1e26d-1e7a-4bf8-a19b-5a9a3ff42a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. -0. -0. -0.]\n",
      " [ 0.  1.  0. -0.]\n",
      " [ 0. -0.  1.  0.]\n",
      " [ 0. -0. -0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "M = np.random.normal(size=(4, 4))\n",
    "M_inv = np.linalg.inv(M)\n",
    "print(np.round(M.dot(M_inv), 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83656fcf-7f86-450d-8e55-2702fd04ca32",
   "metadata": {},
   "source": [
    "Let's try to find the invert of a matrix that is all ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20651112-5e4d-4f61-a995-a41b114b2d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd1ee1d-4af8-4827-8efd-dbe61af71948",
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m M \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m M_inv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/linalg/linalg.py:545\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    543\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    544\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 545\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/linalg/linalg.py:88\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "M = np.ones((4, 4))\n",
    "M_inv = np.linalg.inv(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b4d5b-92df-4501-8ba8-c6f12b2d724e",
   "metadata": {},
   "source": [
    "And `numpy` throws an error for us, telling us the matrix is singular.\n",
    "\n",
    "There is no inverse for this matrix, there's no matrix that can be dotted with $\\mathbf M$ to return the identity. Another way to show this is to calculate the **determinant** of $\\mathbf M$, which is a property of the matrix. If the determinant is 0, there is no inverse for that matrix, and it is singular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c40d55-9afa-4e5b-9b49-fe1a057c34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e3a25-2a84-484a-b3f3-60aab1c43b09",
   "metadata": {},
   "source": [
    "### What is the matrix Rank?\n",
    "\n",
    "Matrix rank is a very useful. It tells us how many of our columns or rows in our matrix are independent. If you have a matrix with rank equal to the smallest dimension of your matrix (i.e. in a `4x3` matrix or a `3x4` matrix that would be `3`) then you have a **full rank** matrix. This would mean all of the rows and columns are linearly independent of each other.\n",
    "\n",
    "In our context, a matrix of regressors with rank equal to the number of columns, then all the regressors are linearly independent. \n",
    "\n",
    "Below we have a matrix of ones with shape `4x3`. It has rank 1 because the rows are all linearly dependent (in fact they are identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0000c-5346-4055-9905-84d3496a86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.ones((4, 3))\n",
    "print(M)\n",
    "print('rank:', np.linalg.matrix_rank(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd711fee-4eec-44a8-afae-57b149ac1415",
   "metadata": {},
   "source": [
    "Similarly a matrix with columns of 1s, 2s and 3s has rank 1 because they can all be specified by the same vector multiplied by different scalar weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1483667f-f691-47cc-8db9-8ff456d360c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.ones((4, 3)) * [1, 2, 3]\n",
    "print(M)\n",
    "print('rank:', np.linalg.matrix_rank(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1ce3a-d089-4284-ac9c-3ef8ba282187",
   "metadata": {},
   "source": [
    "If we instead make a matrix with ones and zeros in different locations, we find a rank 3 matrix, which is a full rank matrix. All of these rows are linearly independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d111c-f933-416b-bd82-7383a3503127",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.diag(np.ones(4))[:, :-1]\n",
    "print(M)\n",
    "print('rank:', np.linalg.matrix_rank(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2fedf-1a6f-43f9-a969-d06119a6b31a",
   "metadata": {},
   "source": [
    "## Solving systems of linear equations\n",
    "\n",
    "Now we can write down a linear system of equations which describe our data. What we want to do in our everyday science is, **write down a linear model and obtain the best fitting weights, given some data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10bf37-0878-4ac2-a96b-e988f554c648",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "\n",
    "Let's imagine that I have some data $\\mathbf y$, and let's imagine for now that I know $\\mathbf y$ is a linear model of $\\mathbf x$, and that it's a first order polynomial. But this time, let's imagine I don't know the weights $\\mathbf{w}$. Now, we're going to have to solve our linear system of equations to find $\\mathbf w$.\n",
    "\n",
    "Just a reminder, we have values of \"x\" that we know map to values of \"y\", we in this case want to know the two values of \"w\" that give us y. Written out as a set of equations this is\n",
    "\n",
    "$y_0 = w_0 * x_0 + w_1$\n",
    "\n",
    "$y_1 = w_0 * x_1 + w_1$\n",
    "\n",
    "$y_2 = w_0 * x_2 + w_1$\n",
    "\n",
    "...\n",
    "\n",
    "$y_i = w_0 * x_i + w_1$\n",
    "\n",
    "Which we can express as\n",
    "\n",
    "$\\mathbf y = \\mathbf A \\cdot \\mathbf w$\n",
    "\n",
    "$ \\mathbf y = \\begin{bmatrix} x_0 & 1 \\\\ x_1 & 1 \\\\ x_2 & 1 \\\\ .. & .. \\\\ x_n & 1 \\end{bmatrix} \\cdot \\mathbf w$\n",
    "\n",
    "\n",
    "So how do we find the values of $\\mathbf w$? Well, for small matrices we could do this by hand, but we're not going to do that here. Let's make some data that we're going to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327957d-b87d-4b89-a638-4a068eda70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ec987-7b38-45b6-8bb2-6595a3a0f6af",
   "metadata": {},
   "source": [
    "For now, let's solve this [small set of linear equations](https://en.wikipedia.org/wiki/System_of_linear_equations):\n",
    "\n",
    "\\begin{cases}3x+2y-z=1\\\\2x-2y+4z=-2\\\\-x+{\\frac {1}{2}}y-z=0\\end{cases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee045d32-0e37-4b7d-a4cf-806df1f78d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up y vector\n",
    "y = np.asarray([1, -2, 0])\n",
    "# Set up design matrix\n",
    "A = np.asarray([[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]])\n",
    "# Use `numpy` linalg solve to find the answer\n",
    "np.linalg.solve(A, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80976d49-b3ed-4f4a-b1dd-bcfb02351412",
   "metadata": {},
   "source": [
    "In the above cell we've set up our system of linear equations and used `numpy`'s `linalg.solve` to find $x$, $y$, \n",
    "and $z$ respectively. \n",
    "\n",
    "This is great! Let's apply it to something that's a bit closer to what we need to do in astronomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30d4dc-75a7-4d71-a39c-4c1c5fe7172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200 # the number of data points\n",
    "m = 4 # the order of our polynomial\n",
    "x = np.linspace(-1, 1, n)\n",
    "A = np.vstack([x**idx for idx in range(m)]).T\n",
    "\n",
    "# In this step, I'm going to make some fake data for us to fit\n",
    "true_w = np.random.normal(size=m)\n",
    "data = A.dot(true_w) + np.random.normal(0, 0.1, size=x.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(x, data, c='k')\n",
    "ax.set(xlabel='x', ylabel='data', title='Fake Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f72a155-5afa-444a-a859-45e64e326553",
   "metadata": {},
   "source": [
    "I generated this data with a matrix $\\mathbf A$ and randomly generated some \"true\" value for $\\mathbf w$. Now we'd like to solve the equation \n",
    "\n",
    "$\\mathbf y = \\mathbf A \\cdot \\mathbf w$\n",
    "\n",
    "There are lots of ways to solve sets of linear equations, and I recommend you take a quick look at this point at [Python Programming and Numerical Methods - A Guide for Engineers and Scientists \"Solutions to Systems of Linear Equations\" section](https://pythonnumericalmethods.berkeley.edu/notebooks/chapter14.04-Solutions-to-Systems-of-Linear-Equations.html)) for some examples of how this \"solving\" happens.\n",
    "\n",
    "Luckily for us, `numpy` has built in some of the solvers that are discussed above! We can use the `solve` function! `solve` will use methods such as LU decomposition to find $\\mathbf{w}$ given $\\mathbf{A}$ and $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e82bc7-6bed-4908-b6a6-899a2b6fea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.solve(A, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b86675-45d2-4618-aa56-4559a79ca62b",
   "metadata": {},
   "source": [
    "Oh dear. No we can't.\n",
    "\n",
    "We can't solve this system of equations as we've written it because $\\mathbf A$ is not square. All the methods above A to be square. (In our description of linear models this means we need the same number of linear equations as we have coefficients to find.)\n",
    "\n",
    "We're going to have to try to do something else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed900387-3fae-47b9-9767-427a36053d7f",
   "metadata": {},
   "source": [
    "<div class=\"admonition\">\n",
    "<p class=\"admonition-title\">The Take Aways</p>\n",
    "    <p> In this notebook you should have seen that <ul>\n",
    "    <li>1) you can build a linear model, and represent it as matrices </li>\n",
    "    <li>2) it is possible to solve linear models in Python using `numpy` functions </li>\n",
    "    <li>3) we can write a simple linear model for test data, but we can only solve it if the design matrix is square. </li></ul></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
